"""
Automotive Bot with LangChain and Chroma Vector Database
Handles conversational flows and retrieval integration for automotive queries
"""

import os
from typing import List, Dict, Any
from dotenv import load_dotenv
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Load environment variables
load_dotenv()

# Configuration from environment variables (same as faq_bot.py)
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://aiportalapi.stu-platform.live/jpe")
MODEL = os.getenv("MODEL_NAME", "GPT-4o-mini")
MAX_TOKENS = int(os.getenv("MAX_TOKENS", "200"))
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.5"))

# Retry configuration
RETRY_ATTEMPTS = int(os.getenv("RETRY_ATTEMPTS", "3"))
RETRY_WAIT_MIN = int(os.getenv("RETRY_WAIT_MIN", "1"))
RETRY_WAIT_MAX = int(os.getenv("RETRY_WAIT_MAX", "10"))

try:
    import chromadb
    from chromadb.config import Settings
    import openai
    
    # Initialize OpenAI client with same config as faq_bot.py
    openai_client = openai.OpenAI(
        base_url=OPENAI_BASE_URL,
        api_key=os.getenv("OPENAI_API_KEY")
    )
    
    # Try to import LangChain components
    try:
        from langchain.embeddings import OpenAIEmbeddings
        from langchain.vectorstores import Chroma
        from langchain.chat_models import ChatOpenAI
        from langchain.chains import ConversationalRetrievalChain
        from langchain.memory import ConversationBufferWindowMemory
        from langchain.prompts import PromptTemplate
        from langchain.schema import Document
        LANGCHAIN_AVAILABLE = True
    except ImportError:
        print("âš ï¸ LangChain not available. Using simple fallback mode.")
        LANGCHAIN_AVAILABLE = False
        
except ImportError as e:
    print(f"âš ï¸ Some dependencies not available: {e}")
    LANGCHAIN_AVAILABLE = False
    openai_client = None

# Initialize ChromaDB client
try:
    chroma_client = chromadb.PersistentClient(path=os.getenv("CHROMA_DB_PATH", "./chroma_db"))
except:
    chroma_client = None

class AutomotiveBot:
    def __init__(self):
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.memory = None
        self.qa_chain = None
        self.conversation_history = []
        self.initialize_components()
    
    def initialize_components(self):
        """Initialize components - LangChain if available, otherwise fallback"""
        try:
            if LANGCHAIN_AVAILABLE and chroma_client and openai_client:
                self._initialize_langchain()
            else:
                self._initialize_fallback()
                
        except Exception as e:
            print(f"Error initializing AutomotiveBot: {str(e)}")
            self._initialize_fallback()
    
    def _initialize_langchain(self):
        """Initialize with full LangChain components"""
        # Initialize embeddings
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=OPENAI_BASE_URL
        )
        
        # Initialize vector store
        self.vectorstore = Chroma(
            client=chroma_client,
            collection_name="automotive_knowledge",
            embedding_function=self.embeddings
        )
        
        # Initialize LLM
        self.llm = ChatOpenAI(
            model_name=MODEL,
            temperature=TEMPERATURE,
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=OPENAI_BASE_URL
        )
        
        # Initialize memory
        self.memory = ConversationBufferWindowMemory(
            k=5,  # Remember last 5 exchanges
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # Create custom prompt template
        prompt_template = """
        Báº¡n lÃ  má»™t chuyÃªn gia tÆ° váº¥n Ã´ tÃ´ chuyÃªn nghiá»‡p vÃ  thÃ¢n thiá»‡n. HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn kiáº¿n thá»©c sau:

        Context: {context}

        Chat History: {chat_history}

        Question: {question}

        HÆ°á»›ng dáº«n tráº£ lá»i:
        - Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t má»™t cÃ¡ch tá»± nhiÃªn vÃ  thÃ¢n thiá»‡n
        - Sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c
        - Náº¿u khÃ´ng cÃ³ thÃ´ng tin trong context, hÃ£y nÃ³i rÃµ vÃ  Ä‘Æ°a ra lá»i khuyÃªn chung
        - Cung cáº¥p thÃ´ng tin chi tiáº¿t vÃ  há»¯u Ã­ch
        - CÃ³ thá»ƒ Ä‘á» xuáº¥t thÃªm cÃ¢u há»i liÃªn quan

        Answer:"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "chat_history", "question"]
        )
        
        # Initialize conversational retrieval chain
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 4}
            ),
            memory=self.memory,
            combine_docs_chain_kwargs={"prompt": PROMPT},
            return_source_documents=True,
            verbose=True
        )
        
        print("âœ… Automotive Bot initialized with LangChain")
    
    def _initialize_fallback(self):
        """Initialize fallback mode without LangChain"""
        self.qa_chain = None
        self.conversation_history = []
        print("âš ï¸ Automotive Bot running in fallback mode (no LangChain)")
    
    @retry(
        stop=stop_after_attempt(RETRY_ATTEMPTS),
        wait=wait_exponential(multiplier=1, min=RETRY_WAIT_MIN, max=RETRY_WAIT_MAX),
        retry=retry_if_exception_type((Exception,)),
        reraise=True
    )
    def _call_openai_direct(self, messages):
        """Direct OpenAI API call for fallback mode"""
        if not openai_client:
            raise Exception("OpenAI client not available")
            
        response = openai_client.chat.completions.create(
            model=MODEL,
            messages=messages,
            max_tokens=MAX_TOKENS,
            temperature=TEMPERATURE
        )
        return response.choices[0].message.content.strip()
    
    def get_response(self, question: str) -> Dict[str, Any]:
        """Get response from the automotive bot"""
        try:
            if self.qa_chain:
                # Use LangChain mode
                result = self.qa_chain({"question": question})
                
                # Format source documents
                sources = []
                if result.get("source_documents"):
                    for i, doc in enumerate(result["source_documents"][:3]):  # Top 3 sources
                        sources.append({
                            "content": doc.page_content[:200] + "...",
                            "metadata": doc.metadata
                        })
                
                return {
                    "answer": result["answer"],
                    "sources": sources,
                    "error": False,
                    "mode": "langchain"
                }
            else:
                # Use fallback mode with direct OpenAI API
                return self._get_fallback_response(question)
                
        except Exception as e:
            return {
                "answer": f"âŒ Lá»—i khi xá»­ lÃ½ cÃ¢u há»i: {str(e)}",
                "sources": [],
                "error": True,
                "mode": "error"
            }
    
    def _get_fallback_response(self, question: str) -> Dict[str, Any]:
        """Fallback response using direct OpenAI API"""
        try:
            # Build conversation context
            messages = [
                {
                    "role": "system", 
                    "content": """Báº¡n lÃ  má»™t chuyÃªn gia tÆ° váº¥n Ã´ tÃ´ chuyÃªn nghiá»‡p vÃ  thÃ¢n thiá»‡n. 
                    
HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i vá»:
- ThÃ´ng tin xe hÆ¡i, cÃ´ng nghá»‡ Ã´ tÃ´
- TÆ° váº¥n mua xe, so sÃ¡nh xe
- Báº£o dÆ°á»¡ng vÃ  sá»­a chá»¯a xe
- LÃ¡i xe an toÃ n
- Xu hÆ°á»›ng ngÃ nh Ã´ tÃ´

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t má»™t cÃ¡ch tá»± nhiÃªn, chi tiáº¿t vÃ  há»¯u Ã­ch."""
                }
            ]
            
            # Add conversation history (last 4 exchanges)
            for msg in self.conversation_history[-8:]:  # 4 exchanges = 8 messages
                messages.append(msg)
            
            # Add current question
            messages.append({"role": "user", "content": question})
            
            # Get response
            answer = self._call_openai_direct(messages)
            
            # Update conversation history
            self.conversation_history.append({"role": "user", "content": question})
            self.conversation_history.append({"role": "assistant", "content": answer})
            
            # Keep history manageable
            if len(self.conversation_history) > 20:
                self.conversation_history = self.conversation_history[-20:]
            
            return {
                "answer": answer,
                "sources": [],
                "error": False,
                "mode": "fallback"
            }
            
        except Exception as e:
            return {
                "answer": f"âŒ Lá»—i khi gá»i OpenAI API: {str(e)}",
                "sources": [],
                "error": True,
                "mode": "fallback_error"
            }
    
    def reset_conversation(self):
        """Reset conversation memory"""
        if self.memory:
            self.memory.clear()
        if self.conversation_history:
            self.conversation_history.clear()
    
    def get_conversation_info(self) -> Dict[str, Any]:
        """Get current conversation information"""
        try:
            if self.memory:
                # LangChain mode
                history = self.memory.chat_memory.messages
                return {
                    "message_count": len(history),
                    "status": "LangChain Active" if len(history) > 0 else "LangChain Empty"
                }
            else:
                # Fallback mode
                return {
                    "message_count": len(self.conversation_history) // 2,  # Each exchange = 2 messages
                    "status": "Fallback Active" if len(self.conversation_history) > 0 else "Fallback Empty"
                }
        except Exception as e:
            return {"message_count": 0, "status": f"Error: {str(e)}"}

# Global instance
automotive_bot = AutomotiveBot()

def get_automotive_response(question: str) -> str:
    """Get response from automotive bot"""
    result = automotive_bot.get_response(question)
    
    if result["error"]:
        return result["answer"]
    
    # Format response with sources and mode info
    response = result["answer"]
    
    # Add mode indicator
    mode_indicator = {
        "langchain": "ðŸ§  LangChain + ChromaDB",
        "fallback": "âš¡ Direct OpenAI API",
        "error": "âŒ Error Mode"
    }.get(result.get("mode", "unknown"), "â“ Unknown Mode")
    
    if result.get("sources"):
        response += f"\n\nðŸ“š **Nguá»“n tham kháº£o ({mode_indicator}):**\n"
        for i, source in enumerate(result["sources"], 1):
            response += f"{i}. {source['content']}\n"
    else:
        response += f"\n\nðŸ¤– *Powered by {mode_indicator}*"
    
    return response

def reset_automotive_conversation():
    """Reset automotive bot conversation"""
    automotive_bot.reset_conversation()

def get_automotive_info() -> Dict[str, Any]:
    """Get automotive bot conversation info"""
    return automotive_bot.get_conversation_info()
