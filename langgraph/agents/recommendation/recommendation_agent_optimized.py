"""
Optimized Car Recommendation Agent
Fast version with reduced LLM calls and better caching.
"""

from typing import Dict, List, Any, Optional
from chat_state import ChatState
from services import get_azure_llm, get_vectordb
from .car_database import VALID_PURPOSES, VALID_PRIORITIES, VALID_BRAND_ORIGINS
import json
import re
import time
from functools import lru_cache


class OptimizedCarRecommendationAgent:
    """
    Optimized car recommendation agent with reduced latency.
    """
    
    def __init__(self):
        self.vectordb = get_vectordb()
        self.llm = get_azure_llm()
        # Cache for common queries
        self._query_cache = {}
        
    @lru_cache(maxsize=100)
    def _cached_criteria_extraction(self, question_hash: str, question: str) -> str:
        """Cache LLM responses for common criteria extraction."""
        extraction_prompt = f"""
        Tr√≠ch xu·∫•t th√¥ng tin mua xe t·ª´ c√¢u h·ªèi: "{question}"
        
        Ch·ªâ tr·∫£ v·ªÅ JSON v·ªõi c√°c tr∆∞·ªùng sau (d√πng null n·∫øu kh√¥ng c√≥):
        {{"budget_max": <s·ªë ti·ªÅn ho·∫∑c null>, "purposes": [<t·ª´: {VALID_PURPOSES}>], "priorities": [<t·ª´: {VALID_PRIORITIES}>], "brand_preference": "<t·ª´: {VALID_BRAND_ORIGINS} ho·∫∑c null>", "passengers": <s·ªë ng∆∞·ªùi ho·∫∑c null>}}
        """
        
        response = self.llm.invoke(extraction_prompt)
        return response.content.strip()
    
    def extract_user_criteria_fast(self, question: str) -> Dict[str, Any]:
        """Fast criteria extraction with caching and fallback."""
        question_lower = question.lower()
        
        # Quick keyword-based extraction first
        criteria = self._quick_keyword_extraction(question_lower)
        
        # Only use LLM if we need complex analysis
        if not any([criteria["budget_max"], criteria["purposes"], criteria["brand_preference"]]):
            try:
                question_hash = str(hash(question))
                llm_response = self._cached_criteria_extraction(question_hash, question)
                
                # Simple JSON extraction
                if "{" in llm_response and "}" in llm_response:
                    json_str = llm_response[llm_response.find("{"):llm_response.rfind("}")+1]
                    llm_criteria = json.loads(json_str)
                    criteria.update({k: v for k, v in llm_criteria.items() if v})
            except:
                pass  # Use keyword extraction fallback
                
        return criteria
    
    def _quick_keyword_extraction(self, question_lower: str) -> Dict[str, Any]:
        """Ultra-fast keyword-based extraction."""
        criteria = {
            "budget_max": None,
            "purposes": [],
            "priorities": [],
            "brand_preference": None,
            "passengers": None
        }
        
        # Budget extraction (Vietnamese patterns)
        budget_patterns = [
            r'(\d+)\s*t·ª∑',  # 1 t·ª∑, 1.5 t·ª∑
            r'(\d+)\s*tri·ªáu',  # 800 tri·ªáu
            r'd∆∞·ªõi\s*(\d+)\s*t·ª∑',  # d∆∞·ªõi 1 t·ª∑
            r'trong\s*t·∫ßm\s*(\d+)\s*t·ª∑',  # trong t·∫ßm 1 t·ª∑
        ]
        
        for pattern in budget_patterns:
            match = re.search(pattern, question_lower)
            if match:
                amount = int(match.group(1))
                if 'tri·ªáu' in pattern:
                    criteria["budget_max"] = amount * 1000000
                elif 't·ª∑' in pattern:
                    criteria["budget_max"] = amount * 1000000000
                break
        
        # Purpose extraction (Vietnamese)
        purpose_map = {
            "gia ƒë√¨nh": "family",
            "family": "family", 
            "ƒëi l√†m": "daily_commute",
            "commute": "daily_commute",
            "kinh doanh": "business",
            "business": "business",
            "du l·ªãch": "leisure",
            "weekend": "leisure"
        }
        
        for vn_term, purpose in purpose_map.items():
            if vn_term in question_lower:
                criteria["purposes"].append(purpose)
        
        # Brand extraction (Vietnamese)
        brand_map = {
            "nh·∫≠t": "Japanese",
            "japanese": "Japanese",
            "h√†n": "Korean", 
            "korean": "Korean",
            "ƒë·ª©c": "German",
            "german": "German",
            "m·ªπ": "American",
            "american": "American"
        }
        
        for brand_term, brand in brand_map.items():
            if brand_term in question_lower:
                criteria["brand_preference"] = brand
                break
                
        return criteria
    
    def query_cars_optimized(self, criteria: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Optimized ChromaDB query with better search terms."""
        cache_key = str(sorted(criteria.items()))
        if cache_key in self._query_cache:
            return self._query_cache[cache_key]
        
        # Build targeted query
        query_terms = []
        
        # Budget terms
        budget = criteria.get("budget_max", 0)
        if budget:
            if budget < 500000000:  # < 500M VND
                query_terms.append("affordable economical budget")
            elif budget < 1500000000:  # < 1.5B VND  
                query_terms.append("mid-range value practical")
            else:
                query_terms.append("luxury premium high-end")
        
        # Purpose terms
        purposes = criteria.get("purposes", [])
        if "family" in purposes:
            query_terms.append("family sedan SUV spacious safe")
        if "daily_commute" in purposes:
            query_terms.append("fuel efficient compact reliable")
        if "business" in purposes:
            query_terms.append("professional luxury sedan")
            
        # Brand terms
        brand = criteria.get("brand_preference")
        if brand:
            query_terms.append(brand.lower())
            
        query = " ".join(query_terms) if query_terms else "car automobile vehicle"
        
        # Reduced k for faster processing
        try:
            results = self.vectordb.similarity_search_with_score(query, k=6)
            car_data = [{
                "content": doc.page_content[:1000],  # Truncate for speed
                "metadata": doc.metadata,
                "score": score
            } for doc, score in results]
            
            # Cache result
            self._query_cache[cache_key] = car_data
            return car_data
            
        except Exception as e:
            print(f"ChromaDB query error: {e}")
            return []
    
    def generate_fast_recommendation(self, question: str, car_data: List[Dict], criteria: Dict) -> str:
        """Single LLM call for fast recommendation generation."""
        if not car_data:
            return self._get_quick_fallback()
            
        # Combine top car data efficiently
        top_cars_text = "\n\n".join([
            f"Car {i+1}: {item['content'][:500]}"  # Limit text per car
            for i, item in enumerate(car_data[:3])  # Only top 3
        ])
        
        # Streamlined prompt for faster processing
        prompt = f"""
        Nhu c·∫ßu c·ªßa ng∆∞·ªùi d√πng: {question}
        Ng√¢n s√°ch: {criteria.get('budget_max', 'kh√¥ng x√°c ƒë·ªãnh')}
        M·ª•c ƒë√≠ch s·ª≠ d·ª•ng: {criteria.get('purposes', [])}
        
        C√°c xe c√≥ s·∫µn:
        {top_cars_text}
        
        H√£y g·ª£i √Ω 3 xe ph√π h·ª£p theo ƒë·ªãnh d·∫°ng sau:
        
        üöó **Top 3 G·ª£i √ù Xe Cho B·∫°n:**
        
        **1. [T√™n xe]** - [Gi√° b√°n]
        ‚úÖ T·∫°i sao ph√π h·ª£p: [l√Ω do ng·∫Øn g·ªçn]
        üìä Th√¥ng s·ªë ch√≠nh: [t√≠nh nƒÉng n·ªïi b·∫≠t]
        
        **2. [T√™n xe]** - [Gi√° b√°n] 
        ‚úÖ T·∫°i sao ph√π h·ª£p: [l√Ω do ng·∫Øn g·ªçn]
        üìä Th√¥ng s·ªë ch√≠nh: [t√≠nh nƒÉng n·ªïi b·∫≠t]
        
        **3. [T√™n xe]** - [Gi√° b√°n]
        ‚úÖ T·∫°i sao ph√π h·ª£p: [l√Ω do ng·∫Øn g·ªçn] 
        üìä Th√¥ng s·ªë ch√≠nh: [t√≠nh nƒÉng n·ªïi b·∫≠t]
        
        **üí° L·ª±a ch·ªçn h√†ng ƒë·∫ßu c·ªßa t√¥i:** [#1 v·ªõi l√Ω do ng·∫Øn]
        
        Tr·∫£ l·ªùi ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát.
        """
        
        try:
            response = self.llm.invoke(prompt)
            return response.content
        except Exception as e:
            return self._format_emergency_response(car_data)
    
    def _get_quick_fallback(self) -> str:
        """Quick fallback when no data available."""
        return """
        üöó **C·∫ßn th√™m th√¥ng tin ƒë·ªÉ g·ª£i √Ω xe ph√π h·ª£p:**
        
        üìã Vui l√≤ng cho bi·∫øt:
        ‚Ä¢ **Ng√¢n s√°ch**: Trong t·∫ßm bao nhi√™u? (VD: 800 tri·ªáu, 1.2 t·ª∑)
        ‚Ä¢ **M·ª•c ƒë√≠ch**: Gia ƒë√¨nh, ƒëi l√†m, kinh doanh?
        ‚Ä¢ **S·ªë ng∆∞·ªùi**: C·∫ßn xe m·∫•y ch·ªó ng·ªìi?
        
        T√¥i s·∫Ω g·ª£i √Ω xe ph√π h·ª£p ngay! üöÄ
        """
    
    def _format_emergency_response(self, car_data: List[Dict]) -> str:
        """Emergency formatting without LLM."""
        if not car_data:
            return self._get_quick_fallback()
            
        response = "üöó **G·ª£i √Ω xe ph√π h·ª£p:**\n\n"
        for i, car in enumerate(car_data[:3], 1):
            content = car["content"][:200]  # Brief excerpt
            response += f"**{i}. Xe ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t**\n"
            response += f"üìã {content}...\n"
            response += f"‚≠ê ƒêi·ªÉm ph√π h·ª£p: {1 - car['score']:.2f}\n\n"
            
        return response + "\nüí¨ B·∫°n mu·ªën bi·∫øt th√™m chi ti·∫øt xe n√†o?"
    
    def process_recommendation_fast(self, state: ChatState) -> ChatState:
        """Main optimized processing method."""
        start_time = time.time()
        question = state["question"]
        
        try:
            # Step 1: Fast criteria extraction
            criteria = self.extract_user_criteria_fast(question)
            
            # Step 2: Optimized database query
            car_data = self.query_cars_optimized(criteria)
            
            # Step 3: Single LLM call for recommendation
            response = self.generate_fast_recommendation(question, car_data, criteria)
            
            # Add performance info in debug mode
            elapsed = time.time() - start_time
            if elapsed > 3:  # Log slow queries
                print(f"Recommendation took {elapsed:.2f}s - consider further optimization")
                
            return {**state, "answer": response}
            
        except Exception as e:
            print(f"Fast recommendation error: {e}")
            fallback_response = self._get_quick_fallback()
            return {**state, "answer": fallback_response}


# Create optimized instance
optimized_car_agent = OptimizedCarRecommendationAgent()


def recommend_car_fast(state: ChatState) -> ChatState:
    """Fast entry point for optimized car recommendations."""
    return optimized_car_agent.process_recommendation_fast(state)
